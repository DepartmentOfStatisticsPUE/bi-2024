---
title: "Symulacje na potrzeby projektu z badań internetowych"
author: "Maciej Beręsewicz"
execute: 
  eval: true
  warning: false
  message: false
format:
  html:
    df_print: paged
    self-contained: true
    table-of-contents: true
    number-sections: false
editor_options: 
  markdown: 
    wrap: 100
---

# Symulacje

Każda grupa projektowa przesyła kod poszczególnych symulacji przez moodle do *11.05 (23:59)*.

Wymagania dot. pliku:

-   plik powiniem nazywać się: *grupa-NR* i mieć jedno z następujących rozszerzeń:
    `.R, .RMD, .QMD, .PY, .IPYNB, .JL`,
-   plik powiniem zawierać komentarze abym mógł zrozumieć co się tam dzieje,
-   jeżeli plik wykorzystuje biblioteki należy je załadować na samym początku,
-   W razie pytań proszę o kontakt,
-   dla każdej symulacji ustawić `set.seed(2024)`.

## Symulacja 1

### Informacje o symulacji

-   $N=100,000$ -- wielkość populacji.
-   Liczba powtórzeń symulacji 500.

### Zmienne pomocnicze

-   wygenerować dwie zmienne $X_1$ i $X_2$ niezależnie z $N(1,1)$ i utworzyć następującą macierz
    (dane umowne)

$$
\mathbf{X}=
\begin{bmatrix}
1 & -0.234 & 0.325 \\
1 & 0.434 & -0.244 \\
... & ... & -1.24 \\
1 & -0.532 &  1.45 
\end{bmatrix}
$$

### Zmienna celu

-   Dwie zmienne celu $Y_1$ i $Y_2$

-   Ciągła: $$
          Y_1 = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon
    $$ gdzie $\mathbf{\beta}=(1, 1, -1)$ i $\epsilon\sim N(0,1)$.

-   Binarna: $$
    Y_2 \sim \text{Ber}(\pi_Y(X))\; \text{gdzie}\; \text{logit}(\pi_Y(X)) = -1 + 2 X_2
    $$ gdzie $\text{Ber}()$ oznacza rozkład Bernoulliego.

### Selekcja do próby nielosowej ($S_A$)

-   Selekcja do próby nielosowej (uwaga wielkość próby będzie losowa!):

$$
\text{logit}(\pi_{B,i}) = 3.5 + \mathbf{\alpha}^T\mathbf{X}_i
$$

gdzie $\mathbf{\alpha}=(0,3,-3)^T$.

### Selekcja do próby losowej ($S_B$)

-   Selekcja do próby losowej ($S_B$) o liczebności 1,000 przez losowanie proste ze zwracaniem.

## Symulacja 2

### Informacje o symulacji

-   $N=10,000$ -- wielkość populacji.
-   Liczba powtórzeń symulacji 500.

### Zmienne pomocnicze

-   $p=5$ -- liczba zmiennych $X_p$ gdzie $p-1$ zmiennych generowanych jest z rozkładu $N(0,1)$
    (pierwsza kolumna tej macierzy to stała, wyraz wolny). Poniżej przykład jak powinna wygladać ta
    macierz (liczby umowne, wymiar macierzy $N \times p$).

$$
X=
\begin{bmatrix}
1 & -0.234 & ... &  0.325\\
1 & 0.434 & ...  &  -0.244 \\
... & ... & ...  & -1.24 \\
1 & -0.532 & ...  & 1.45 
\end{bmatrix}
$$

### Zmienna celu

-   Dwie zmienne celu $Y_1$ i $Y_2$

-   Ciągła: $$
          Y_1 = 1 + X_{4i} + X_{5i} + \epsilon
    $$ gdzie $\epsilon\sim N(0,1)$.

-   Binarna: $$
    Y_2 \sim \text{Ber}(\pi_Y(X))\; \text{with}\; \text{logit}(\pi_Y(X)) = \mathbf{\beta}^TX_i
    $$ gdzie $\mathbf{\beta}=(1,0,0,-1,1)^T$ oraz $\text{Ber}$ oznacza rozkład
    Bernoulliego.

### Selekcja do próby nielosowej ($S_A$)

-   Selekcja do próby nielosowej ($S_A$, uwaga wielkość próby będzie losowa!):

$$
\text{logit}(\pi_{B,i}) = 3.5 - \sin(X_{2i} + X_{3i}) - X_{4i} - X_{5i}
$$

Przykładowo $X_{2i}$ oznacza 3 kolumnę macierzy $\mathbf{X}$

### Selekcja do próby losowej ($S_B$)

-   Selekcja do próby losowej ($S_B$) o liczebności 1,000:
-   losowanie proste z warstwowaniem
-   2 warstwy utworzone według następującej kombinacji 2 zmiennych:

$$
\begin{cases}
\text{Warstwa 1} & = X_1 < 0.5\\
\text{Warstwa 2} & = X_1 \geq 0.5\\
\end{cases}
$$

-   liczebność próby w poszczególnych warstwach wynosi: 700, 300,

-   prawdopodobieństwo inkluzji dla każdej warstwy wynosi:
    $\pi_w=(\frac{7}{100}, \frac{3}{100})$  (błąd!!! powinno być $\pi_w=(\frac{n_1}{N_1}, \frac{n_2}{N_2})$ gdzie $n_1$ to wielkość próby dla pierwszej klasy, a $N_1$ to wielkość populacji tej klasy),

-   każdej wylosowanej jednostce należy przypisać wagę $d_i$ będącą odwrotnością inkluzji warstwy, w
    której się znajduje.
    



## Symulacja 3

### Informacje o symulacji

-   $N=10,000$ -- wielkość populacji.
-   Liczba powtórzeń symulacji 500.

### Zmienne pomocnicze

-   $p=15$ -- liczba zmiennych $X_p$ gdzie $p-1$ zmiennych generowanych jest z rozkładu $N(0,1)$
    (pierwsza kolumna tej macierzy to stała, wyraz wolny). Poniżej przykład jak powinna wygladać ta
    macierz (liczby umowne, wymiar macierzy $N\times p$).

$$
X=
\begin{bmatrix}
1 & -0.234 & ... &  0.325 \\
1 & 0.434 & ...  &  -0.244\\
... & ... & ...  & -1.24 \\
1 & -0.532 & ...  & 1.45 
\end{bmatrix}
$$

### Zmienna celu

-   Dwie zmienne celu $Y_1$ i $Y_2$

-   Ciągła: $$
          Y_1 = 1 + \exp(3\sin(\mathbf{\beta}^T\mathbf{X}_i)) + X_{5i} + X_{6i} + \epsilon
    $$ gdzie $\mathbf{\beta}=(1,0,0,1/2,-1/2,1/2,-1/2,0,...,0)^T$ i $\epsilon\sim N(0,1)$.

-   Binarna: $$
    Y_2 \sim \text{Ber}(\pi_Y(X))\; \text{with}\; \text{logit}(\pi_Y(X)) = \mathbf{\beta}^TX_i
    $$ gdzie $\mathbf{\beta}=(1,0,0,-1,1,-1,1,0,...,0)^T$ oraz $\text{Ber}$ oznacza rozkład
    Bernoulliego.

### Selekcja do próby nielosowej ($S_A$)

-   Selekcja do próby nielosowej ($S_A$, uwaga wielkość próby będzie losowa!):

$$
\text{logit}(\pi_{B,i}) = (3.5 + \mathbf{\alpha}^T(\log(\mathbf{X}_i)^2) - \sin(X_{3i} + X_{4i}) - X_{5i} - X_{6i})
$$

gdzie $\mathbf{\alpha}=(0,0,0,3,-3,3,-3,0,...,0)^T$. Przykładowo $X_{3i}$ oznacza 3 kolumnę macierzy
X.

### Selekcja do próby losowej ($S_B$)

-   Selekcja do próby losowej ($S_B$) o liczebności 1,000:
-   losowanie proste z warstwowaniem
-   4 warstwy utworzone według następującej kombinacji 2 zmiennych:

$$
\begin{cases}
\text{Warstwa 1} & = X_2 < 0.5 & X_4 < 0,\\
\text{Warstwa 2} & = X_2 \geq 0.5 & X_4 < 0,\\
\text{Warstwa 3} & = X_2 < 0.5 & X_4 \geq 0,\\
\text{Warstwa 4} & = X_2 \geq 0.5 & X_4 \geq 0,\\
\end{cases}
$$

-   liczebność próby w poszczególnych warstwach wynosi: 100, 200, 300, 400,

-   prawdopodobieństwo inkluzji dla każdej warstwy wynosi:
    $\pi_w=(\frac{1}{100}, \frac{2}{100}, \frac{3}{100}, \frac{4}{100})$ (błąd!!! powinno być $\pi_w=(\frac{n_1}{N_1}, \frac{n_2}{N_2}, \frac{n_3}{N_3}, \frac{n_4}{N_4})$ gdzie $n_1$ to wielkość próby dla pierwszej klasy, a $N_1$ to wielkość populacji tej klasy),

-   każdej wylosowanej jednostce należy przypisać wagę $d_i$ będącą odwrotnością inkluzji warstwy, w
    której się znajduje.


# Raporty dla każdej symulacji

W każdym przebiegu:

-   należy zapisać średnią $Y_1$ i $Y_2$ z próby nielosowej $S_A$,
-   należy zadeklarować obiekt `svydesign` dla próby losowej $S_B$ uwzględniając argument `strata`
    określający warstwę oraz wyznaczyć oszacowanie średniej $Y_1$ i $Y_2$.

Po każdej symulacji należy zaraportować dla $Y_1$ i $Y_2$ oddzielnie symulacyjne:

-   Obciązenie

$$
\text{Bias} = \bar{\hat{\mu}}_Y - \mu_Y
$$ gdzie $\bar{\hat{\mu}}_Y= \left(\frac{1}{R} \sum_{r=1}^{R} \hat{\mu}_{r,Y}\right)$

-   Wariancję

$$
\text{Var} = \sqrt{\frac{\left(\sum_{r=1}^{R} \hat{\mu}_{r,Y} - \bar{\hat{\mu}}_Y \right)^2}{n}}
$$ + RMSE

$$
\text{RMSE} = \sqrt{\text{Bias}^2 + \text{Var}} 
$$


# Rozwiązania

## Pakiety

```{r}
library(survey)
library(sampling)
```

## Kod do symulacji 1

### Kod w R 

```{r}
set.seed(2024)       ## odtwarzalnosc
N <- 100000         ## wielkosc populacji
n_B <- 1000         ## wielkosć próby losowej
R <- 500            ## liczba iteracji
X <- cbind(1, rnorm(N, 1,1), rnorm(N, 1,1))         ## macierz X
betas <- c(1,1,-1)                                  ## bety w regresji dla Y1
alphas <- c(0,3,-3)                                 ## alfy dla selekcji
epsilon <- rnorm(N)                                 ## reszty dla regresji
Y1 <- as.numeric(X %*% as.matrix(betas)) + epsilon  ## regresja Y1
Y2 <- rbinom(N, 1, plogis(-1 + 2*X[,2]))            ## regresja Y2
s_A_sel <- as.numeric(plogis(3.5 + X %*% as.numeric(alphas)))  ## selekcja do próby S_A
pop_data <- data.frame(X,Y1,Y2, s_A_sel)                       ## zbior danych
results  <- matrix(data=0,nrow = R, ncol = 4)                  ## macierz z wynikami
colnames(results) <- c("nonprob_Y1","nonprob_Y2", "prob_Y1", "prob_Y2")         ## nazwy kolumn

for (r in 1:R) {
  S_A <- pop_data[which(rbinom(N, 1, s_A_sel) == 1),]         ## losujemy jednostki do próby S_A
  S_B <- pop_data[sample(1:N, n_B),]                          ## losujemy jednostki do próby S_B
  results[r, 1:2] <- colMeans(S_A[, c("Y1", "Y2")])           ## wyznaczamy średnie na podstawie S_A
  results[r, 3:4] <- colMeans(S_B[, c("Y1", "Y2")])           ## wyznaczamy średnie na podstawie S_B
}

## obciązenuie
bias <- colMeans(results) - c(colMeans(pop_data[, c("Y1", "Y2")]),colMeans(pop_data[, c("Y1", "Y2")]))
var <- apply(results, 2, var)                   ## wariancja
rmse <- sqrt(bias^2 + var)                      ## rmse

data.frame(bias=bias, var=var, rmse=rmse)       ## raport
```

### Kod w Python

```{python}
import numpy as np
import pandas as pd
from scipy.special import expit

np.random.seed(2024)

N = 100000  # size of population
n_B = 1000  # size of random sample
R = 500     # number of iterations

X = np.column_stack((np.ones(N), np.random.normal(size=N, loc=1, scale=1),
                      np.random.normal(size=N, loc=1, scale=1)))

betas = np.array([1, 1, -1])  # coefficients for Y1 regression
alphas = np.array([0, 3, -3])   # coefficients for selection
epsilon = np.random.normal(size=N)  # residuals for regression

Y1 = X.dot(betas) + epsilon  # regression Y1
Y2 = np.random.binomial(1, expit(-1 + 2 * X[:, 1]), size=N)  # regression Y2
s_A_sel = expit(3.5 + X.dot(alphas))  # selection for S_A

pop_data = pd.DataFrame(np.hstack((X, Y1.reshape(-1, 1), Y2.reshape(-1, 1), s_A_sel.reshape(-1, 1))), columns=['X0', 'X1', 'X2', 'Y1', 'Y2', 's_A_sel'])

results = np.zeros((R, 4))

for r in range(R):
    S_A = pop_data[np.random.binomial(1, s_A_sel, size=N) == 1]
    S_B = pop_data.sample(n=n_B)
    results[r, :2] = S_A[['Y1', 'Y2']].mean()
    results[r, 2:] = S_B[['Y1', 'Y2']].mean()

bias = results.mean(axis=0) - np.array([pop_data['Y1'].mean(), pop_data['Y2'].mean(), pop_data['Y1'].mean(), pop_data['Y2'].mean()])
var = results.var(axis=0)
rmse = np.sqrt(bias**2 + var)

pd.DataFrame({'Bias': bias, 'Var': var, 'RMSE': rmse})
```



## Kod do symulacji 2

```{r, eval=FALSE}
set.seed(2024)       ## odtwarzalnosc
N <- 10000          ## wielkosc populacji
n_B <- c(700, 300)  ## wielkosć próby losowej
R <- 500            ## liczba iteracji
X <- cbind(1, rnorm(N), rnorm(N), rnorm(N), rnorm(N))        ## macierz X
betas <- c(1, 0,0, 1,-1)                                     ## bety w regresji dla Y2
epsilon <- rnorm(N)                                          ## reszty dla regresji
Y1 <- 1 + X[, 4] + X[, 5] + epsilon                                  ## regresja Y1
Y2 <- rbinom(N, 1, plogis(X %*% as.matrix(betas)))                   ## regresja Y2
s_A_sel <- as.numeric(plogis(3.5 - sin(X[,2]+X[,3]) + X[,4]+X[,5]))  ## selekcja do próby S_A
X_warstwa <- ifelse(X[,2] < 0.5, "warstwa1", "warstwa2")
pop_data <- data.frame(X,Y1,Y2, X_warstwa, s_A_sel)                             ## zbior danych
results  <- matrix(data=0,nrow = R, ncol = 4)                        ## macierz z wynikami
colnames(results) <- c("nonprob_Y1","nonprob_Y2", "prob_Y1", "prob_Y2")         ## nazwy kolumn

for (r in 1:R) {
  S_A <- pop_data[which(rbinom(N, 1, s_A_sel) == 1),]         ## losujemy jednostki do próby S_A
  
  S_B <- sampling::strata(pop_data, "X_warstwa", size=c(700, 300), method="srswor")
  S_B <- svydesign(ids = ~1, prob = ~Prob, strata = ~ Stratum, data = getdata(pop_data, S_B))

  results[r, 1:2] <- colMeans(S_A[, c("Y1", "Y2")])           ## wyznaczamy średnie na podstawie S_A
  results[r, 3:4] <- svymean(~Y1+Y2, S_B)[1:2]                ## wyznaczamy średnie na podstawie S_B
}

## obciązenuie
bias <- colMeans(results) - c(colMeans(pop_data[, c("Y1", "Y2")]),colMeans(pop_data[, c("Y1", "Y2")]))
var <- apply(results, 2, var)                   ## wariancja
rmse <- sqrt(bias^2 + var)                      ## rmse

data.frame(bias=bias, var=var, rmse=rmse)       ## raport
```

## Kod do symulacji 3

```{r}
set.seed(2024)
N <- 10000
n_B <- 1000
R <- 500
p <- 14
X <- cbind(1, matrix(rnorm(N*14), N))
betas1 <- c(1,0,0,1/2,-1/2,1/2,-1/2, rep(0, p-6))
betas2 <- c(1,0,0,-1,1,-1,1, rep(0, p-6))
alphas <- c(1,0,0,-1,1,-1,1, rep(0, p-6))
epsilon <- rnorm(N)

Y1 <- 1 + exp(3*sin(as.numeric(X %*% as.matrix(betas1)))) + X[,5] + X[,6] + epsilon 
Y2 <- rbinom(N, 1, plogis(as.numeric(X %*% as.matrix(betas2))))
s_A_sel <- as.numeric(plogis(3.5  + log(X^2) %*% as.matrix(alphas) - sin(X[,3]+X[,4]) - X[,5] - X[,6]))

X_warstwa <- numeric(N)
X_warstwa[X[,2] < 0.5 & X[,4] < 0] <- 1
X_warstwa[X[,2] >= 0.5 & X[,4] < 0] <- 2
X_warstwa[X[,2] < 0.5 & X[,4] > 0] <- 3
X_warstwa[X[,2] >= 0.5 & X[,4] > 0] <- 4

pop_data <- data.frame(X,Y1,Y2, s_A_sel, X_warstwa)
results  <- matrix(data=0,nrow = R, ncol = 4)

colnames(results) <- c("nonprob_Y1","nonprob_Y2", "prob_Y1", "prob_Y2")

for (r in 1:R) {
  
  S_A <- pop_data[which(rbinom(N, 1, s_A_sel) == 1),]
  
  S_B <- sampling::strata(pop_data, "X_warstwa", size=c(1:4)*100, method="srswor")
  S_B <- svydesign(ids = ~1, prob = ~Prob, strata = ~ Stratum, data = getdata(pop_data, S_B))

  results[r, 1:2] <- colMeans(S_A[, c("Y1", "Y2")])
  results[r, 3:4] <- svymean(~Y1+Y2, S_B)[1:2]
}

bias <- colMeans(results) - c(colMeans(pop_data[, c("Y1", "Y2")]),colMeans(pop_data[, c("Y1", "Y2")]))
var <- apply(results, 2, var)
rmse <- sqrt(bias^2 + var)

data.frame(bias=bias, var=var, rmse=rmse)
```
